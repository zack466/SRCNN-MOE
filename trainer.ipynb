{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading .dll files for GPU b/c for some reason TF isn't finding them through PATH (Windows 10)\n",
    "#    -assumes that Cuda Toolkit v10.0.130 is installed in 'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\'\n",
    "#    -assumes that nvcuda.dll is in 'C:\\System32\\'\n",
    "import ctypes\n",
    "hllDll = ctypes.WinDLL(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.0\\\\bin\\\\cudart64_100.dll\")\n",
    "tllDll = ctypes.WinDLL(\"C:\\\\tools\\\\cuda\\\\bin\\\\cudnn64_7.dll\")\n",
    "cllDll = ctypes.WinDLL(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.0\\\\bin\\\\cublas64_100.dll\")\n",
    "ullDll = ctypes.WinDLL(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.0\\\\extras\\\\CUPTI\\\\libx64\\\\cupti64_100.dll\")\n",
    "nllDll = ctypes.WinDLL(\"C:\\\\Windows\\\\System32\\\\nvcuda.dll\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import cv2\n",
    "from conv import MOE\n",
    "import random\n",
    "from data_gen import read_data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import io\n",
    "import csv\n",
    "import ipykernel\n",
    "import collections\n",
    "import six\n",
    "from trainvisout import train_schedule\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zack4\\Anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:281: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
      "WARNING:tensorflow:From C:\\Users\\zack4\\Anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "Loaded weights:  weights-E3D225\n",
      "Weights not found\n",
      "Loaded weights:  weights-E3D225\n",
      "Weights not found\n",
      "Loaded weights:  weights-E3D225\n",
      "Weights not found\n",
      "Loaded weights:  weights-E3D225\n",
      "Weights not found\n",
      "Loaded weights:  weights-E3D225\n",
      "Weights not found\n"
     ]
    }
   ],
   "source": [
    "#image batch shape is (16,128,128,3)\n",
    "batch_size = 256\n",
    "def get_params():\n",
    "    data_start = input(\"Which dataset to use? (0-318) \")\n",
    "    data_end = input(\"Which dataset to use? (0-318) \")\n",
    "    test_start = input(\"Which testset to use? (0-35) \")\n",
    "    test_end = input(\"Which testset to use? (0-35) \")\n",
    "    num_experts = input(\"Number of total experts: \")\n",
    "    top_k = input(\"Top K: \")\n",
    "    optimizer = input(\"Optimizer (SGD, Adam): \")\n",
    "    loss = input(\"Loss (psnr, ssim, mssiml1): \")\n",
    "    scale = input(\"Scale (2,3,4): \")\n",
    "    epochs = input(\"Number of epochs: \")\n",
    "    name = input(\"Name of run: \")\n",
    "    behavior = input(\"'train', 'evalu', 'predict', or 'all'? \")\n",
    "    return {\"data_start\":data_start,\n",
    "            \"data_end\":data_end,\n",
    "            \"test_start\":test_start,\n",
    "            \"test_end\":test_end,\n",
    "            \"num_experts\":num_experts,\n",
    "            \"top_k\":top_k,\n",
    "            \"optimizer\":optimizer,\n",
    "            \"loss\":loss,\n",
    "            \"name\":name,\n",
    "            \"epochs\":epochs,\n",
    "            \"scale\":scale,\n",
    "            \"behavior\":behavior}\n",
    "\n",
    "\n",
    "def ssim_loss(image,label):\n",
    "    #negative of SSIM b/c TF will try to minimize loss\n",
    "    return tf.reduce_mean(tf.image.ssim(image,label,1.)) * -1\n",
    "\n",
    "def mssim_loss(image,label):\n",
    "    #negative of SSIM b/c TF will try to minimize loss\n",
    "    return tf.reduce_mean(tf.image.ssim_multiscale(image,label,1.)) * -1\n",
    "\n",
    "def psnr_loss(image,label):\n",
    "    #negative of PSNR b/c TF will try to minimize loss\n",
    "    return tf.reduce_mean(tf.image.psnr(image,label,max_val=1.0)) * -1\n",
    "\n",
    "def psnr_loss1(image,label):\n",
    "    def log10(num):\n",
    "        num = tf.dtypes.cast(num,tf.float32)\n",
    "        numer = tf.math.log(num)\n",
    "        denom = tf.math.log(tf.constant(10,dtype=numer.dtype))\n",
    "        return numer/denom\n",
    "    mse = tf.reduce_mean(tf.math.squared_difference(image,label))\n",
    "    #mse = tf.reduce_mean(tf.math.squared_difference(image,label),[-3,-2,-1])\n",
    "    \n",
    "    psnr = 20 * log10(1.0) - 10 * log10(mse)\n",
    "    \n",
    "    #psnr = tf.math.subtract(20 * tf.math.log(1.0) / tf.math.log(10.0),np.float32(10 / np.log(10)) * tf.math.log(mse))\n",
    "\n",
    "    return psnr\n",
    "\n",
    "def mse_loss(image,label):\n",
    "    return tf.reduce_mean(tf.keras.losses.MSE(image,label))\n",
    "\n",
    "def load_loss(model_load,full_load):\n",
    "    model_load = tf.dtypes.cast(model_load,tf.float32)\n",
    "    full_load = tf.dtypes.cast(full_load,tf.float32)\n",
    "    model_sum = tf.reduce_mean(model_load,axis=0)\n",
    "    perfect_sum = tf.reduce_mean(full_load,axis=0)\n",
    "    return tf.keras.losses.MSE(model_sum,perfect_sum)\n",
    "\n",
    "def date_time():\n",
    "    return str(datetime.now().time())\n",
    "\n",
    "def mssiml1_loss(image,label):\n",
    "    \"\"\"see https://arxiv.org/abs/1511.08861\"\"\"\n",
    "    \n",
    "    def _fspecial_gauss(size, sigma, img1, img2):\n",
    "        \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function.\n",
    "          Copied directly from TF image_ops, low-level ops replaced with functional counterparts\n",
    "        \"\"\"\n",
    "        size = tf.convert_to_tensor(size, tf.int32)\n",
    "        sigma = tf.convert_to_tensor(sigma)\n",
    "\n",
    "        coords = tf.dtypes.cast(tf.range(size), sigma.dtype)\n",
    "        coords -= tf.dtypes.cast(size - 1, sigma.dtype) / 2.0\n",
    "\n",
    "        g = tf.math.square(coords)\n",
    "        g *= -0.5 / tf.math.square(sigma)\n",
    "\n",
    "        g = tf.reshape(g, shape=[1, -1]) + tf.reshape(g, shape=[-1, 1])\n",
    "        g = tf.reshape(g, shape=[1, -1])  # For tf.nn.softmax().\n",
    "        g = tf.nn.softmax(g)\n",
    "        return tf.reshape(g, shape=[size, size, 1, 1])\n",
    "\n",
    "    def reducer(x,kernel):\n",
    "        \"\"\"Copied directly from TF image_ops, low-level ops replaced with functional counterparts\"\"\"\n",
    "        shape = tf.shape(x)\n",
    "        x = tf.reshape(x, shape=tf.concat([[-1], shape[-3:]], 0))\n",
    "        y = tf.nn.depthwise_conv2d(x, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        return tf.reshape(\n",
    "            y, tf.concat([shape[:-3], tf.shape(y)[1:]], 0))\n",
    "    \n",
    "    alpha = 0.84\n",
    "    shape1, shape2 = tf.shape_n([image, label])\n",
    "    \n",
    "    kernel = _fspecial_gauss(11,0.2,shape1,shape2)\n",
    "    kernel = tf.tile(kernel, multiples=[1, 1, shape1[-1], 1])\n",
    "    l1 = tf.reduce_mean(tf.multiply(tf.math.abs(image-label),reducer(image,kernel)))\n",
    "    mssim = tf.reduce_mean(tf.image.ssim_multiscale(image,label,1,power_factors=(0.0448,)))\n",
    "    mixed = alpha*mssim - (1-alpha)*l1\n",
    "    return -1 * mixed\n",
    "\n",
    "def load_weights(model_dir):\n",
    "    moe.train_on_batch(x=tf.ones((moe.batch_size,32,32,3)),y=[tf.ones((moe.batch_size,16,16,3)),tf.ones((moe.batch_size,num_experts))])\n",
    "    filelist = sorted(filter(os.path.isfile, os.listdir(model_dir)), key=os.path.getmtime)\n",
    "    filelist.reverse()\n",
    "    for filename in filelist:\n",
    "        if filename[-6:]==\".index\":\n",
    "            moe.load_weights(filename[:-6])\n",
    "            print(\"Loaded weights: \",filename[:-6])\n",
    "            break\n",
    "    print(\"Weights not found\")\n",
    "\n",
    "class TrainCallback(tf.keras.callbacks.CSVLogger):\n",
    "    def __init__(self,**kwargs):\n",
    "        self.epoch = 0\n",
    "        super(TrainCallback,self).__init__(**kwargs)\n",
    "    \n",
    "    def on_train_batch_end(self,batch,logs=None):\n",
    "        if batch==0:\n",
    "            self.epoch += 1\n",
    "        epoch = self.epoch\n",
    "        logs = logs or {}\n",
    "\n",
    "        def handle_value(k):\n",
    "            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n",
    "            if isinstance(k, six.string_types):\n",
    "                return k\n",
    "            elif isinstance(k, collections.abc.Iterable) and not is_zero_dim_ndarray:\n",
    "                return '\"[%s]\"' % (', '.join(map(str, k)))\n",
    "            else:\n",
    "                return k\n",
    "\n",
    "        if self.keys is None:\n",
    "            self.keys = sorted(logs.keys())\n",
    "        \n",
    "        if self.model.stop_training:\n",
    "            # We set NA so that csv parsers do not fail for this last epoch.\n",
    "            logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])\n",
    "\n",
    "        if not self.writer:\n",
    "\n",
    "            class CustomDialect(csv.excel):\n",
    "                delimiter = self.sep\n",
    "\n",
    "            fieldnames = ['epoch'] + self.keys\n",
    "\n",
    "            self.writer = csv.DictWriter(\n",
    "                self.csv_file,\n",
    "                fieldnames=fieldnames,\n",
    "                dialect=CustomDialect)\n",
    "            if self.append_header:\n",
    "                self.writer.writeheader()\n",
    "\n",
    "        row_dict = collections.OrderedDict({'epoch': epoch})\n",
    "        row_dict.update((key, handle_value(logs[key])) for key in self.keys)\n",
    "        self.writer.writerow(row_dict)\n",
    "        self.csv_file.flush()\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        pass\n",
    "\n",
    "class TestCallback(tf.keras.callbacks.CSVLogger):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(TestCallback,self).__init__(**kwargs)\n",
    "        self.epoch = 0\n",
    "    def on_test_begin(self,logs=None):\n",
    "        super(TestCallback,self).on_train_begin(logs=logs)\n",
    "    def on_test_batch_end(self,batch,logs=None):\n",
    "        if batch==0:\n",
    "            self.epoch += 1\n",
    "        super(TestCallback,self).on_epoch_end(self.epoch,logs=logs)\n",
    "    def on_test_end(self,logs=None):\n",
    "        super(TestCallback,self).on_train_end(logs=None)\n",
    "\n",
    "\n",
    "### build model ###\n",
    "for params in train_schedule:\n",
    "    K.clear_session()\n",
    "    \n",
    "    #params = get_params()\n",
    "\n",
    "    num_experts = int(params[\"num_experts\"])\n",
    "    top_k = int(params[\"top_k\"])\n",
    "    optimizer = params[\"optimizer\"]\n",
    "    if optimizer=='Adam':\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "    elif optimizer=='SGD':\n",
    "        optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "    x = params[\"behavior\"]\n",
    "\n",
    "    behavior = {}\n",
    "    if (x=='all'):\n",
    "        behavior[\"train\"] = True\n",
    "        behavior[\"evalu\"] = True\n",
    "        behavior[\"predict\"] = True\n",
    "        behavior[\"evalu_1\"] = False\n",
    "    elif (x=='train'):\n",
    "        behavior[\"train\"] = True\n",
    "        behavior[\"evalu\"] = False\n",
    "        behavior[\"predict\"] = False\n",
    "        behavior[\"evalu_1\"] = False\n",
    "    elif (x=='evalu'):\n",
    "        behavior[\"train\"] = False\n",
    "        behavior[\"evalu\"] = True\n",
    "        behavior[\"predict\"] = False\n",
    "        behavior[\"evalu_1\"] = False\n",
    "    elif (x=='predict'):\n",
    "        behavior[\"train\"] = False\n",
    "        behavior[\"evalu\"] = False\n",
    "        behavior[\"predict\"] = True\n",
    "        behavior[\"evalu_1\"] = False\n",
    "    elif (x=='evalu_1'):\n",
    "        behavior[\"train\"] = False\n",
    "        behavior[\"evalu\"] = False\n",
    "        behavior[\"predict\"] = False\n",
    "        behavior[\"evalu_1\"] = True\n",
    "\n",
    "    loss = params[\"loss\"]\n",
    "    if loss==\"ssim\":\n",
    "        loss = ssim_loss\n",
    "    elif loss==\"psnr\":\n",
    "        loss = psnr_loss\n",
    "    elif loss==\"mse\":\n",
    "        loss = keras.losses.MSE\n",
    "    elif loss==\"mssim\":\n",
    "        loss = mssim_loss\n",
    "    elif loss==\"mssiml1\":\n",
    "        loss = mssiml1_loss\n",
    "    elif loss==\"ssiml1\":\n",
    "        loss = ssiml1_loss\n",
    "\n",
    "    name = params[\"name\"]\n",
    "    epochs = int(params[\"epochs\"])\n",
    "    scale = int(params[\"scale\"])\n",
    "\n",
    "    batch_size = num_experts * 32\n",
    "    data_start = int(params[\"data_start\"])\n",
    "    data_end = int(params[\"data_end\"]) + 1\n",
    "    test_start = int(params[\"test_start\"])\n",
    "    test_end = int(params[\"test_end\"]) + 1\n",
    "\n",
    "    train_data_dir = \"D:/TestData/train-x\" + str(scale)+\"/\"\n",
    "    test_data_dir = \"D:/TestData/test-x\" + str(scale)+\"/\"\n",
    "    assert os.path.exists(train_data_dir), \"train_data_dir does not exist\"\n",
    "    assert os.path.exists(test_data_dir), \"test_data_dir does not exist\"\n",
    "\n",
    "    model_dir = \"D:/SR-Research/models-N256/\" + name\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    os.chdir(model_dir)\n",
    "\n",
    "    moe = MOE(num_experts,top_k,batch_size,train_data_dir)\n",
    "    if loss==ssim_loss or loss==mssim_loss or loss==mssiml1_loss or loss==ssiml1_loss:\n",
    "        moe.compile(optimizer=optimizer,\n",
    "                    loss=[loss,load_loss],\n",
    "                    loss_weights=[1.0,128.0],\n",
    "                    metrics=[[ssim_loss,psnr_loss1,mse_loss],[]])\n",
    "    elif loss==psnr_loss:\n",
    "        moe.compile(optimizer=optimizer,loss=[loss,load_loss],loss_weights=[0.8,0.2])\n",
    "    elif loss==keras.losses.MSE:\n",
    "        moe.compile(optimizer=optimizer,loss=[loss,load_loss],loss_weights=[0.8,0.2])\n",
    "\n",
    "    #init = tf.zeros((batch_size,32,32,3))\n",
    "    #init = tf.dtypes.cast(init,tf.float32)\n",
    "    #moe.predict(init)\n",
    "    moe.build((batch_size,32,32,3))\n",
    "    if name in os.listdir(\"D:\\SR-Research\\models-N256\\\\\"):\n",
    "            load_weights(model_dir)\n",
    "    #if behavior[\"train\"]:\n",
    "    #    while True:\n",
    "    #        a = input(\"First time training? \")\n",
    "    #        if (a=='y'):\n",
    "    #            break\n",
    "    #        elif (a=='n'):\n",
    "    #            load_weights()\n",
    "    #            break\n",
    "    #else:\n",
    "    #    load_weights()\n",
    "    \n",
    "    if behavior[\"train\"]:\n",
    "        print(date_time() + \", Training start: \",params[\"name\"])\n",
    "        ##code for training on data\n",
    "        csv_logger = TrainCallback(filename='training.log',append=True)\n",
    "\n",
    "        train_sets = os.listdir(train_data_dir)\n",
    "        for i in range(epochs):\n",
    "            for datanum in range(data_start,data_end):\n",
    "                train_set = train_sets[datanum]\n",
    "                images, labels = read_data(train_data_dir + train_set)\n",
    "\n",
    "                #cuts off images off the end of the batch so that it is divisible by 16\n",
    "                if (len(images) % batch_size != 0):\n",
    "                    images = images[:-(len(images)%batch_size)]\n",
    "                    labels = labels[:-(len(labels)%batch_size)]\n",
    "\n",
    "                labels = np.array(tf.image.central_crop(labels,1/2))\n",
    "\n",
    "                load = np.full((len(images),num_experts),(top_k/num_experts))\n",
    "\n",
    "                moe.fit(x=images,y=[labels,load],batch_size=batch_size,epochs=1,callbacks=[csv_logger],verbose=0)\n",
    "\n",
    "                #saves weights each training session\n",
    "                if datanum%25 == 0:\n",
    "                    moe.save_weights(\"weights-E\"+str(i)+\"D\"+str(datanum), save_format=\"tf\")\n",
    "        print(date_time() + \", Training finish: \",params[\"name\"])\n",
    "\n",
    "    if behavior[\"evalu\"]:\n",
    "        print(date_time() + \", Eval start: \",params[\"name\"])\n",
    "        ##code for evaluating on test sets\n",
    "        csv_logger_test = TestCallback(filename='testing.log',append=True)\n",
    "        test_sets = os.listdir(test_data_dir)\n",
    "        for testnum in range(test_start,test_end):\n",
    "            test_set = test_sets[testnum]\n",
    "            images, labels = read_data(test_data_dir + test_set)\n",
    "\n",
    "            #cuts off images off the end of the batch so that it is divisible by 16\n",
    "            if (len(images) % batch_size != 0):\n",
    "                images = images[:-(len(images)%batch_size)]\n",
    "                labels = labels[:-(len(labels)%batch_size)]\n",
    "\n",
    "            labels = np.array(tf.image.central_crop(labels,1/2))\n",
    "\n",
    "            load = np.full((len(images),num_experts),(top_k/num_experts))\n",
    "\n",
    "            moe.evaluate(x=images,y=[labels,load],batch_size=batch_size,callbacks=[csv_logger_test],verbose=0)\n",
    "        print(date_time() + \", Eval finished: \",params[\"name\"])\n",
    "    if behavior[\"evalu_1\"]:\n",
    "        K.clear_session()\n",
    "        csv_logger_test = TestCallback(filename='testing_1.log',append=True)\n",
    "        inputs = keras.Input(shape=(batch_size,16,16,3))\n",
    "        model = keras.Model(inputs=inputs,outputs=inputs)\n",
    "        model.compile(optimizer=optimizer,\n",
    "                    loss=loss,\n",
    "                    metrics=[ssim_loss,psnr_loss1,mse_loss])\n",
    "        test_sets = os.listdir(test_data_dir)\n",
    "        for testnum in range(test_start,test_end):\n",
    "            test_set = test_sets[testnum]\n",
    "            images,labels = read_data(test_data_dir + test_set)\n",
    "            images = np.array(tf.image.central_crop(images,1/2))\n",
    "            labels = np.array(tf.image.central_crop(labels,1/2))\n",
    "            model.evaluate(x=images,y=labels,batch_size=batch_size,callbacks=[csv_logger_test],verbose=0)\n",
    "            \n",
    "###update to 128\n",
    "    if behavior[\"predict\"]:\n",
    "        train_sets = os.listdir(train_data_dir)\n",
    "        def vis_out(num=0):\n",
    "            images, labels = read_data(train_data_dir + train_sets[num])\n",
    "            images = images[-1*batch_size:]\n",
    "            labels = labels[-1*batch_size:]\n",
    "            predictions, _ = moe.predict(images,batch_size=batch_size,steps=1)\n",
    "            if not os.path.exists(\"visual_outputs/small/\"):\n",
    "                os.makedirs(\"visual_outputs/small/\")\n",
    "            for num in range(batch_size):\n",
    "                img = np.array(tf.image.central_crop(images[num] * 255,1/2))\n",
    "                label = np.array(tf.image.central_crop(labels[num] * 255,1/2))\n",
    "                pred = predictions[num] * 255\n",
    "                #cv2.imwrite(\"visual_outputs/small/image\"+str(num)+\".png\",img)\n",
    "                #cv2.imwrite(\"visual_outputs/small/label\"+str(num)+\".png\",label)\n",
    "                cv2.imwrite(\"visual_outputs/small/prediction\"+str(num)+\".png\",pred)\n",
    "        def rebuild_img(scale=2,nums=[2,6,51,336,619,150,449,427]): #add 2 back in\n",
    "            for num in nums:\n",
    "                num = str(num).zfill(4)\n",
    "                hrpath = \"F:\\div2k\\DIV2K_train_HR\\\\\"\n",
    "                hrimg = cv2.imread(hrpath + num + \".png\")\n",
    "                #hrimg = cv2.imread(\"F:/div2k/DIV2K_train_LR_bicubic/X4/0002x4.png\") #testing image\n",
    "                height, width, channels = hrimg.shape\n",
    "                \n",
    "                if height % 256 != 0:\n",
    "                    height -= height % 256\n",
    "                height += 16\n",
    "                if width % 256 != 0:\n",
    "                    width -= width % 256\n",
    "                width += 16\n",
    "                hrimg = hrimg[0:height,0:width]\n",
    "\n",
    "                height, width, channels = hrimg.shape\n",
    "\n",
    "                lrimg = cv2.cvtColor(hrimg,cv2.COLOR_BGR2RGB)\n",
    "                lrimg = np.ndarray.astype(lrimg,'float32')\n",
    "                lrimg /= 255.\n",
    "                lrimg = cv2.resize(lrimg,(int(width/2.),int(height/2.)))\n",
    "                lrimg = cv2.resize(lrimg,(width,height),interpolation=cv2.INTER_CUBIC)\n",
    "                predlist = []\n",
    "                patches = np.empty((0,32,32,3))\n",
    "                for h in range(1,height//16):\n",
    "                    for w in range(1,width//16):\n",
    "                        patch = lrimg[(h-1)*16:(h+1)*16,(w-1)*16:(w+1)*16] #32x32 patch\n",
    "                        patch = np.expand_dims(patch,axis=0)\n",
    "                        predlist.append(patch)\n",
    "                        #patches = np.concatenate((patches,patch),axis=0)\n",
    "                    #print(h,\" done\")\n",
    "                    #preds = moe.predict(patches,batch_size=16)[0] #shape (16,16,3)\n",
    "                    #across = np.empty((16,0,3))\n",
    "                    #for i in range(len(preds)):\n",
    "                    #    pred = preds[i:i+1]\n",
    "                    #    pred = pred[0]\n",
    "                    #    across = np.concatenate((across,pred),axis=1)\n",
    "                    #newimg = np.concatenate((newimg,across),axis=0)\n",
    "                    #print(str(round(newimg.shape[0]/(size-32),2)) + \"%\")\n",
    "                #patches in shape (num_patches, 32, 32 3)\n",
    "                patches = np.concatenate(predlist,axis=0)\n",
    "                counter = len(patches)\n",
    "                hx = height//16 - 1\n",
    "                wx = width//16 - 1\n",
    "                \n",
    "                if (counter%batch_size)==0:\n",
    "                    extras = np.empty((batch_size,32,32,3))\n",
    "                    num_extras = len(extras)\n",
    "                else:\n",
    "                    extras = patches[-1*(counter%batch_size):]\n",
    "                    num_extras = len(extras)\n",
    "                    extras = np.concatenate((extras,np.tile(np.empty((1,32,32,3)),(batch_size-num_extras,1,1,1))),axis=0)\n",
    "                    patches = patches[:-1*(counter%batch_size)]\n",
    "                \n",
    "                \n",
    "                patch_pred = moe.predict(patches,batch_size=batch_size)[0]\n",
    "                extra_pred = moe.predict(extras,batch_size=batch_size)[0]\n",
    "                extra_pred = extra_pred[:-1*(batch_size-num_extras)]\n",
    "                \n",
    "                preds = np.concatenate((patch_pred,extra_pred),axis=0)\n",
    "                #shape (wx*hx,16,16,32)\n",
    "                newimg = None\n",
    "                for h in range(hx):\n",
    "                    across = across = np.empty((16,0,3))\n",
    "                    for w in range(wx):\n",
    "                        \n",
    "                        x = preds[h * wx + w:h * wx + w + 1]\n",
    "                        \n",
    "                        x = x[0]\n",
    "                        \n",
    "                        across = np.concatenate((across,x),axis=1)\n",
    "                    \n",
    "                    try:\n",
    "                        if newimg==None:\n",
    "                            newimg = np.empty((0,across.shape[1],3),dtype=np.float32)\n",
    "                    except:\n",
    "                        pass\n",
    "                    #print(across.shape)\n",
    "                    #print(newimg.shape)\n",
    "                    newimg = np.concatenate((newimg,across),axis=0)\n",
    "\n",
    "                newimg *= 255\n",
    "                newimg = newimg.astype(np.float32)\n",
    "                lrimg = lrimg.astype(np.float32)\n",
    "                newimg = cv2.cvtColor(newimg,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                lrimg = cv2.cvtColor(lrimg,cv2.COLOR_BGR2RGB)\n",
    "                lrimg *= 255\n",
    "                if not os.path.exists(\"visual_outputs/large/\"):\n",
    "                    os.makedirs(\"visual_outputs/large/\")\n",
    "                assert cv2.imwrite(\"visual_outputs/large/image\"+str(num).zfill(4)+\".png\",lrimg)\n",
    "                assert cv2.imwrite(\"visual_outputs/large/label\"+str(num).zfill(4)+\".png\",hrimg)\n",
    "                assert cv2.imwrite(\"visual_outputs/large/prediction\"+str(num).zfill(4)+\".png\",newimg)\n",
    "        vis_out()\n",
    "        #rebuild_img(scale=scale)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

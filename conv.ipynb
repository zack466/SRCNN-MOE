{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading .dll files for GPU b/c for some reason TF isn't finding them through PATH (Windows 10)\n",
    "#    -assumes that Cuda Toolkit v10.0.130 is installed in 'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\'\n",
    "#    -assumes that nvcuda.dll is in 'C:\\System32\\'\n",
    "import ctypes\n",
    "hllDll = ctypes.WinDLL(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.0\\\\bin\\\\cudart64_100.dll\")\n",
    "tllDll = ctypes.WinDLL(\"C:\\\\tools\\\\cuda\\\\bin\\\\cudnn64_7.dll\")\n",
    "cllDll = ctypes.WinDLL(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.0\\\\bin\\\\cublas64_100.dll\")\n",
    "ullDll = ctypes.WinDLL(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.0\\\\extras\\\\CUPTI\\\\libx64\\\\cupti64_100.dll\")\n",
    "nllDll = ctypes.WinDLL(\"C:\\\\Windows\\\\System32\\\\nvcuda.dll\")\n",
    "\n",
    "#all necessary imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model,Input\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Layer,InputLayer\n",
    "from tensorflow.keras.activations import softmax\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#tensorboard  in Jupyter Notebook\n",
    "#%load_ext tensorboard\n",
    "\n",
    "#set image formatting for TF\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "#to prevent memory allocation fails on GPU\n",
    "#    -assumes one GPU in system being used for training\n",
    "tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0],True)\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(Layer):\n",
    "    \"\"\"Multiple Conv2D layers wrapped into a single \"expert\" layer.\"\"\"\n",
    "    \n",
    "    def __init__(self,num,**kwargs):\n",
    "        \"\"\"\n",
    "        Initialize an Expert layer\n",
    "        \n",
    "        num  -- the assigned \"number\" for the expert, can be from 0 to num_experts - 1\n",
    "        \"\"\"\n",
    "        \n",
    "        ### initialize layer ###\n",
    "        super(Expert,self).__init__(**kwargs)\n",
    "        \n",
    "        #grabs the corresponding input from the gate layer\n",
    "        self.split_1 = Splitter(0)\n",
    "        self.split_2 = Splitter(num)\n",
    "        \n",
    "        #applies convolutions according to https://arxiv.org/abs/1501.00092\n",
    "        self.conv_3 = optConv2D(64,9,strides=(1,1),padding='valid',activation='relu',dynamic=True)\n",
    "        self.conv_4 = optConv2D(32,5,strides=(1,1),padding='valid',activation='relu',dynamic=True)\n",
    "        self.conv_5 = optConv2D(3,5,strides=(1,1),padding='valid',activation='relu',dynamic=True)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        out = self.split_1(inputs)\n",
    "        out = self.split_2(out)\n",
    "        out = self.conv_3(out)\n",
    "        out = self.conv_4(out)\n",
    "        out = self.conv_5(out)\n",
    "        return out\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateLayer(Layer):\n",
    "    \"\"\"A GateLayer takes in the original input image and the weights from the Gater.\n",
    "    Input shape: [(16,128,128,3),(16,num_experts)]\n",
    "    It outputs the tensors that the experts will receive along with the weigts from the Gater\n",
    "    \"\"\"\n",
    "    def __init__(self,num_experts,k,batch_size,**kwargs):\n",
    "        super(GateLayer, self).__init__(**kwargs)\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        super(GateLayer,self).build(input_shape)\n",
    "        \n",
    "        #a trainable tensor for the input weights\n",
    "        self.ws = self.add_weight(name='ws',\n",
    "                                  dtype=tf.float32,\n",
    "                                  shape=input_shape[1],\n",
    "                                  initializer=tf.initializers.zeros(),\n",
    "                                  trainable=True)\n",
    "         \n",
    "        #a trainable tensor for noise\n",
    "        self.noise_weight = self.add_weight(name='noise_weight',\n",
    "                                            dtype=tf.float32,\n",
    "                                            shape=input_shape[1],\n",
    "                                            initializer=tf.initializers.zeros(),\n",
    "                                            trainable=True)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        \"\"\"\n",
    "        Inputs are in shape [(None,128,128,3),(none,num_experts)].\n",
    "        Processes the weights from the Gater with noisy top_k gating.\n",
    "        Outputs either the original image input or a null tensor to the experts\n",
    "        Outputs the processed weights to the Combine layer\n",
    "        \"\"\"\n",
    "        \n",
    "        ### Processes the weights ###\n",
    "        \n",
    "        # see https://arxiv.org/pdf/1701.06538.pdf\n",
    "        weights = inputs[1]\n",
    "        noisy_top_k = (weights*self.ws) + (K.random_normal(weights.shape)*K.softplus(weights*self.noise_weight))\n",
    "        #noisy_top_k = weights\n",
    "        #creates a binary mask to remove non top_k weights\n",
    "        indices_to_remove = noisy_top_k < tf.math.top_k(noisy_top_k,self.k,sorted=True)[0][..., -1, None]\n",
    "        mask = tf.where(indices_to_remove == False, x=1, y=0)\n",
    "        mask = tf.dtypes.cast(mask,tf.float32)\n",
    "        \n",
    "        #sets all non top_k values to -inf\n",
    "        output_weights = noisy_top_k * mask\n",
    "        output_weights = tf.where(output_weights==0,x=-np.inf,y=output_weights)\n",
    "        \n",
    "        \n",
    "        #applies softmax function\n",
    "        output_weights = K.softmax(output_weights)\n",
    "        \n",
    "        ### Processes the inputs for the experts ###\n",
    "        \n",
    "        # depending on the weights, either passes the original input on to the experts or passes a null tensor (all -1)\n",
    "        output_inputs = []\n",
    "        \n",
    "        input_passthrough = inputs[0]\n",
    "        input_shape = input_passthrough.shape\n",
    "        null = tf.zeros_like(input_passthrough)-1\n",
    "        \n",
    "        #mask is in shape (16,4)\n",
    "        #input_passthrough and null in shape (16,128,128,3)\n",
    "        #want output of [(16,128,128,3) * num_experts], each (1,128,128,3) image is either an input_passthrough or null\n",
    "        counter = 0 #from 0 to 15\n",
    "        unstacked = tf.unstack(mask) #[(1,4) * 16]\n",
    "        for stack in unstacked: # each stack is (1,4)\n",
    "            singles = tf.split(stack,self.num_experts) #[(1),(1),(1),(1)]\n",
    "            for single_val in singles: #[[(1),(1),(1),(1)] * 16]\n",
    "                #replaces each 1 or 0 with the corresponding input or null tensor of shape (1,128,128,3)\n",
    "                if K.all(tf.math.equal(single_val,1)): #if expert is in top_k\n",
    "                    output_inputs.append(tf.gather(input_passthrough,counter))\n",
    "                else: #if not\n",
    "                    output_inputs.append(tf.gather(null,counter))\n",
    "            counter += 1\n",
    "        \n",
    "        #left with output_inputs as a list in shape [(1,128,128,3) * 64]\n",
    "        output_inputs = tf.squeeze(output_inputs)\n",
    "        output_inputs = tf.stack(output_inputs,axis=0) #(64,128,128,3)\n",
    "        output_inputs = tf.split(output_inputs,self.batch_size) #[(4,128,128,3) * 16]\n",
    "        \n",
    "        #grabs the corresponding (1,128,128,3) images from each 16 tensors and stacks them\n",
    "        output_inputs = [tf.stack([tf.gather(output_inputs[i],j) for i in range(self.batch_size)]) for j in range(self.num_experts)]\n",
    "        \n",
    "        #we are left with output_inputs as a list of shape [(16,128,128,3) * num_experts]\n",
    "        \n",
    "        self.outputs = [output_inputs,mask,output_weights]\n",
    "        return self.outputs\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #see call()\n",
    "        baseshape = [[]]\n",
    "        shape = input_shape[0]\n",
    "        for i in range(self.num_experts):\n",
    "            baseshape[0].append(shape)\n",
    "        baseshape.append(input_shape[1])\n",
    "        baseshape.append(input_shape[1])\n",
    "        return baseshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gater(Layer):\n",
    "    \"\"\"Takes in the original image input and generates weights for each expert\"\"\"\n",
    "    \n",
    "    def __init__(self,num_experts,**kwargs):\n",
    "        super(Gater,self).__init__(**kwargs)\n",
    "        \n",
    "        self.num_experts = num_experts\n",
    "        self.conv_1 = Conv2D(4,4,2,activation='relu')\n",
    "        self.conv_2 = Conv2D(4,4,2,activation='relu')\n",
    "        self.flatten_3 = Flatten()\n",
    "        self.dense_4 = Dense(num_experts,activation='relu',name='gater')\n",
    "        self.dense_5 = Dense(num_experts,activation='softmax',name='gater',activity_regularizer=tf.keras.regularizers.l2(0.001))\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.conv_2(out)\n",
    "        out = self.flatten_3(out)\n",
    "        out = self.dense_4(out)\n",
    "        out = self.dense_5(out)\n",
    "        return out\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #essentially (batch_size,num_experts)\n",
    "        return (input_shape[0],self.num_experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(Layer):\n",
    "    \"\"\"Grabs a the corresponding input from gatelayer based on the number of the expert\"\"\"\n",
    "    \n",
    "    def __init__(self,num,**kwargs):\n",
    "        super(Splitter,self).__init__(**kwargs)\n",
    "        self.num = num\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        super(Splitter,self).build(input_shape)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return inputs[self.num]\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape[self.num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combine(Layer):\n",
    "    \"\"\"Combines the expert outputs based on the processed weights from gatelayer\"\"\"\n",
    "    \n",
    "    def __init__(self,num_experts,top_k,batch_size,**kwargs):\n",
    "        super(Combine,self).__init__(**kwargs)\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(Combine,self).build(input_shape)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        experts = inputs[0]\n",
    "        weights = inputs[-1]\n",
    "        weighted_experts = []\n",
    "        counter = 0\n",
    "        for expert in experts:\n",
    "            weight_col = tf.reshape(tf.stack([[tf.gather(weights[j],counter) for j in range(self.batch_size)]]),(self.batch_size,1))\n",
    "            while len(weight_col.shape) < len(expert.shape):\n",
    "                weight_col = tf.expand_dims(weight_col,-1)\n",
    "            weighted_experts.append(tf.multiply(expert,weight_col))\n",
    "            counter += 1\n",
    "        return tf.reduce_sum(weighted_experts,axis=0)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optConv2D(Conv2D):\n",
    "    @tf.function\n",
    "    def call(self,inputs):\n",
    "        null = tf.zeros(tf.shape(inputs))\n",
    "        if K.all(tf.math.equal(inputs,null)):\n",
    "            null = tf.zeros(self.compute_output_shape(inputs.shape)) - 1\n",
    "            return null\n",
    "        else:\n",
    "            return super(optConv2D,self).call(inputs)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return super(optConv2D,self).compute_output_shape(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MOE(Model):\n",
    "    def __init__(self,num_experts,top_k,batch_size,train_data_dir,**kwargs):\n",
    "        super(MOE,self).__init__(**kwargs)\n",
    "        self.train_data_dir = train_data_dir\n",
    "        \n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.inputlayer = InputLayer(input_shape=(32,32,3))\n",
    "\n",
    "        #gater\n",
    "        self.gater = Gater(self.num_experts,input_shape=(32,32,3),name='gater')\n",
    "\n",
    "        self.gate = GateLayer(num_experts=self.num_experts,k=self.top_k,batch_size=self.batch_size,dynamic=True)#([self.inputlayer,self.gater.output])\n",
    "\n",
    "        self.experts = [Expert(i) for i in range(self.num_experts)]\n",
    "        \n",
    "        self.get_weights = Splitter(-1)#(self.gate)\n",
    "        self.get_mask = Splitter(-2)\n",
    "\n",
    "        self.combine = Combine(num_experts=self.num_experts,top_k=self.top_k,batch_size=self.batch_size,dynamic=True)#([self.experts_out,self.weights_out])\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        inputs = self.inputlayer(inputs)\n",
    "        out_gater = self.gater(inputs)\n",
    "        out_gate = self.gate([inputs,out_gater])\n",
    "        \n",
    "        experts = [expert(out_gate) for expert in self.experts]\n",
    "        weights = self.get_weights(out_gate)\n",
    "        mask = self.get_mask(out_gate)\n",
    "        \n",
    "        output = self.combine([experts,weights])\n",
    "        return output,mask\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape,tf.TensorShape([input_shape[0],self.num_experts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
